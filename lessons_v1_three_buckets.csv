lesson_id,course_id,lesson_num,Name,Web URL,Skills,Topics,Tags
az-01,azure-llmops-workshop,1,Lab 01: Introduction to LLMs and Azure AI Services,https://microsoft.github.io/llmops-workshop/labs/lesson_01/lab01.html,Azure AI Studio navigation; Azure OpenAI model deployment; Open-source LLM deployment (Azure); Prompt Playground experimentation; Content Safety configuration; Prompt Flow creation,Foundations & Lifecycle; Deployment & Serving; Safety & Responsible AI; Orchestration & Workflows,Azure; Prompt Flow; Azure AI Studio; Lifecycle; Intro; Hands-on
az-02,azure-llmops-workshop,2,Lab 02: Building LLM Orchestration Flows,https://microsoft.github.io/llmops-workshop/labs/lesson_02/lab02.html,Prompt Flow orchestration; Classification flow build (Prompt Flow); RAG flow build (Prompt Flow + Azure AI Search); Search index setup (Azure AI Search); Flow runtime & connections (Azure OpenAI),Orchestration & Workflows; Retrieval-Augmented Generation (RAG); Prompting & Prompt Management,Azure; Prompt Flow; Azure AI Studio; Azure AI Search; Orchestration; Prompt management
az-03,azure-llmops-workshop,3,Lab 03: Evaluating and Deploying LLMs,https://microsoft.github.io/llmops-workshop/labs/lesson_03/lab03.html,RAG flow evaluation (Prompt Flow); Groundedness evaluation flow; Evaluation dataset preparation; Managed endpoint deployment; Post-deploy validation,Evaluation & Testing; Deployment & Serving; Retrieval-Augmented Generation (RAG),Azure; Prompt Flow; Azure AI Studio; Evaluation; Deployment; RAG
az-04,azure-llmops-workshop,4,Lab 04: Monitoring and Responsible AI,https://microsoft.github.io/llmops-workshop/labs/lesson_04/lab04.html,LLM monitoring enablement; Monitoring metrics preparation; Content Safety tool integration; Conditional safety routing (Prompt Flow); Operational debugging,Monitoring & Observability; Safety & Responsible AI,Azure; Prompt Flow; Monitoring; Observability; Content safety; Hands-on
az-05,azure-llmops-workshop,5,Lab 05: Automating Everything (CI/CD),https://microsoft.github.io/llmops-workshop/labs/lesson_05/lab05.html,GitHub Actions CI/CD setup; Automated PR evaluation pipeline; Golden dataset evaluation gate; CI build + CD deploy (Prompt Flow); Release workflow (branching/versioning),CI/CD & Release Automation; Evaluation & Testing; Deployment & Serving,Azure; Prompt Flow; GitHub Actions; Automation; CI/CD; Hands-on
az-06,azure-llmops-workshop,6,Extra: Benchmarking Azure OpenAI Models,https://microsoft.github.io/llmops-workshop/labs/performance/docs/AOAI_BENCH_TOOL.html,Model benchmarking; Quality/cost tradeoff analysis; Model selection workflow,Evaluation & Testing; Performance & Load Testing,Azure; Azure OpenAI; Performance; Evaluation; Benchmarking
az-07,azure-llmops-workshop,7,Extra: Performance Evaluation / Load Testing,https://microsoft.github.io/llmops-workshop/labs/performance/docs/PERFTEST_CONCEPTS.html,Load testing; Latency/throughput measurement; Scaling analysis,Performance & Load Testing; Deployment & Serving,Azure; Performance; Load testing
gcp-01,gcp-llmops-overview,1,What is LLMOps (overview),https://cloud.google.com/discover/what-is-llmops,LLMOps lifecycle mapping; LLMOps best-practice framing,Foundations & Lifecycle,Google Cloud; Lifecycle; Definition; Conceptual
gcp-02,gcp-llmops-overview,2,Vertex AI Model Registry (lifecycle management),https://docs.cloud.google.com/vertex-ai/docs/model-registry/introduction,Vertex AI Model Registry usage; Model versioning & lineage; Model lifecycle promotion,Model Registry & Versioning; Foundations & Lifecycle,Google Cloud; Vertex AI; Model registry; Versioning
gcp-03,gcp-llmops-overview,3,Vertex AI Pipelines (automation),https://docs.cloud.google.com/vertex-ai/docs/pipelines/introduction,Vertex AI Pipelines orchestration; Pipeline automation; Pipeline parameterization,Orchestration & Workflows; CI/CD & Release Automation,Google Cloud; Vertex AI Pipelines; Pipelines; Automation
gcp-04,gcp-llmops-overview,4,Vertex AI Model Evaluation (overview),https://docs.cloud.google.com/vertex-ai/docs/evaluation/introduction,Vertex AI model evaluation; Metric selection; Model comparison,Evaluation & Testing,Google Cloud; Vertex AI; Evaluation
gcp-05,gcp-llmops-overview,5,Vertex AI Model Monitoring (overview),https://docs.cloud.google.com/vertex-ai/docs/model-monitoring/overview,Vertex AI model monitoring; Drift detection; Alerting configuration,Monitoring & Observability,Google Cloud; Vertex AI; Monitoring
awsw-00,aws-llmops-workshop,1,Lab 0: Register base model (notebook),https://github.com/aws-samples/llmops-workshop/blob/main/lab0-register-base-model.ipynb,SageMaker Model Registry registration; Model package governance; Lifecycle state management,Model Registry & Versioning; Foundations & Lifecycle,AWS; SageMaker; Model registry; Lifecycle; Notebook
awsw-01,aws-llmops-workshop,2,Lab 1: Finetune Llama2 with QLoRA (notebook),https://github.com/aws-samples/llmops-workshop/blob/main/lab1-sagemaker-finetune-llama2-qlora.ipynb,SageMaker fine-tuning job; QLoRA fine-tuning; Model deployment (SageMaker Hosting),Fine-tuning & Post-training; Deployment & Serving,AWS; SageMaker; Post-training; Qlora; Peft; Fine-tuning
awsw-02,aws-llmops-workshop,3,Lab 2: Build SageMaker pipeline for LLM (notebook),https://github.com/aws-samples/llmops-workshop/blob/main/lab2-sagemaker-pipeline-llm.ipynb,SageMaker Pipelines workflow; Training pipeline automation; CI/CD integration (CodePipeline),Orchestration & Workflows; CI/CD & Release Automation; Deployment & Serving,AWS; SageMaker; Pipelines; Automation; Notebook
awsw-03,aws-llmops-workshop,4,Lab 3: Foundation model monitoring (folder),https://github.com/aws-samples/llmops-workshop/tree/main/lab3-sagemaker-fm-monitoring,SageMaker Model Monitor configuration; Monitoring schedule setup; BYOC monitoring,Monitoring & Observability,AWS; SageMaker; Monitoring; Observability
awsw-04,aws-llmops-workshop,5,Lab 4: JumpStart embeddings (notebook),https://github.com/aws-samples/llmops-workshop/blob/main/lab4-sagemaker-jumpstart-embeddings.ipynb,JumpStart embeddings deployment; Embedding generation; Vector store setup (OpenSearch),Retrieval-Augmented Generation (RAG); Deployment & Serving,AWS; SageMaker; OpenSearch; Embeddings; Retrieval; RAG
awsw-05,aws-llmops-workshop,6,Lab 5: Knowledge base chatbot (notebook),https://github.com/aws-samples/llmops-workshop/blob/main/lab5-knowledge-base-chatbot.ipynb,Document ingestion for RAG; LangChain RAG application; Vector DB querying (OpenSearch); Streamlit chatbot UI,Retrieval-Augmented Generation (RAG); Orchestration & Workflows; Deployment & Serving,AWS; LangChain; OpenSearch; Rag; App Integration; RAG
awsb-01,aws-llmops-blog-series,1,LLMOps solution overview (Part I),https://aws.amazon.com/blogs/gametech/operationalize-generative-ai-applications-on-aws-part-i-overview-of-llmops-solution/,LLMOps reference architecture review; Service-to-lifecycle mapping,Foundations & Lifecycle; CI/CD & Release Automation; Deployment & Serving,AWS; Architecture; Lifecycle; Blog
awsb-02,aws-llmops-blog-series,2,SageMaker Clarify: Get started with model evaluations,https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-foundation-model-evaluate-get-started.html,Foundation model evaluation (Clarify); Automatic vs human eval workflows; fmeval-based evaluation,Evaluation & Testing; Safety & Responsible AI,AWS; Evaluation
awsb-03,aws-llmops-blog-series,3,SageMaker Model Monitor (examples index),https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_model_monitor/index.html,Model Monitor setup; LLM monitoring (BYOC); Monitoring schedule operations,Monitoring & Observability,AWS; SageMaker; Monitoring
awsb-04,aws-llmops-blog-series,4,SageMaker Clarify + fmeval (re:Post article),https://repost.aws/articles/ARXsxgLFa7R-uw8TgUo1fDJg/accelerate-foundation-model-evaluation-with-amazon-sagemaker-clarify-and-fmeval,Clarify + fmeval evaluation workflow; Evaluation reporting,Evaluation & Testing,AWS; Evaluation; Tooling; Community
db-01,databricks-llmops,1,Databricks LLMOps overview,https://www.databricks.com/glossary/llmops,LLMOps lifecycle framing (Databricks),Foundations & Lifecycle,Databricks; Definition; Lifecycle; Documentation; Conceptual
db-02,databricks-llmops,2,Evaluate and monitor AI agents (MLflow for GenAI),https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/,Agent evaluation (MLflow); Agent monitoring; Trace logging,Evaluation & Testing; Monitoring & Observability; Orchestration & Workflows,Databricks; Evaluation; Monitoring; Agents
db-03,databricks-llmops,3,Implement LLMOps with Azure Databricks (lab),https://microsoftlearning.github.io/mslearn-databricks/Instructions/Exercises/AI-07-LLMOps.html,LLMOps implementation (Azure Databricks); Experiment/prompt tracking (MLflow); GenAI app deployment; Governance controls,Deployment & Serving; Model Registry & Versioning; Governance & Compliance,Azure; MLflow; Deployment; Tracking; Governance; Hands-on
db-04,databricks-llmops,4,Microsoft Learn module: Implement LLMOps with Azure Databricks,https://learn.microsoft.com/en-us/training/modules/implement-llmops-azure-databricks/,LLMOps implementation patterns (Azure Databricks); Operational lifecycle setup,Foundations & Lifecycle; Deployment & Serving,Azure; Lifecycle; Deployment; Documentation
ibm-01,ibm-llmops,1,IBM: What are Large Language Model Operations (LLMOps)?,https://www.ibm.com/think/topics/llmops,LLMOps definition & lifecycle framing; LLMOps vs MLOps comparison,Foundations & Lifecycle,IBM; Definition; Lifecycle; Conceptual
ibm-02,ibm-llmops,2,IBM watsonx guide (LLMOps framing),https://ibm.github.io/MLOps/watsonx/watsonx/watsonx.pdf,watsonx LLMOps workflow framing; Enterprise governance planning,Foundations & Lifecycle; Governance & Compliance,IBM; Governance; Lifecycle
ibm-03,ibm-llmops,3,IBM community: Build an agentic LLMOps stack with watsonx,https://community.ibm.com/community/user/blogs/patrick-meyer/2025/09/18/build-an-agentic-llmops-stack-with-ibm-watsonx,Agentic workflow design; Agent/tool orchestration; Governance for agentic systems,Orchestration & Workflows; Governance & Compliance,IBM; Agents; Orchestration; Governance; Community
ibm-04,ibm-llmops,4,IBM MLOps guide (governance/risk toolkit mentions),https://ibm.github.io/MLOps/pdf/mlops-guide.pdf,AI governance & risk controls; Compliance/audit processes,Governance & Compliance; Safety & Responsible AI,IBM; Governance; Risk; Documentation
oci-01,oracle-llmops,1,Oracle LLMOps overview,https://www.oracle.com/artificial-intelligence/llmops/,LLMOps overview (Oracle); OCI service mapping for LLMOps,Foundations & Lifecycle,Oracle; Definition; Monitoring; Conceptual
oci-02,oracle-llmops,2,Deploy LLM on OCI Data Science using BYOC (vLLM),https://github.com/oracle-samples/oci-data-science-ai-samples/blob/main/LLM/deploy-llm-byoc.md,BYOC serving deployment (OCI Data Science); Containerized LLM serving (vLLM),Deployment & Serving,Oracle; vLLM; OCI Data Science; Deployment; Serving; Open Models
oci-03,oracle-llmops,3,OCI Observability & Monitoring platform (PDF guide),https://www.oracle.com/a/otn/docs/oracle_distributed_cloud_systems_oci_observability_and_management_v.0.2.pdf,Observability dashboarding; Logs/metrics/traces analysis; Alerting configuration,Monitoring & Observability,Oracle; OCI Observability; Observability; Monitoring; Documentation
oci-04,oracle-llmops,4,Monitoring OCI DevOps performance (Observability & Mgmt),https://www.ateam-oracle.com/monitor-oci-devops-performance-observability-and-management,CI/CD observability; DevOps pipeline performance monitoring,CI/CD & Release Automation; Monitoring & Observability,Oracle; OCI DevOps; CI/CD; Monitoring; Observability
hf-01,hf-open-llmops-primitives,1,TRL (Transformer Reinforcement Learning) docs index,https://huggingface.co/docs/trl/index,RLHF training (TRL); DPO preference optimization; Transformers + TRL integration,Fine-tuning & Post-training,Hugging Face; TRL; Post-training; Rlhf; Dpo; RLHF
hf-02,hf-open-llmops-primitives,2,PEFT docs index,https://huggingface.co/docs/peft/index,PEFT fine-tuning; LoRA adapter training; Adapter merge/export,Fine-tuning & Post-training,Hugging Face; PEFT; LoRA; Peft; Lora; Fine-tuning
hf-03,hf-open-llmops-primitives,3,Accelerate docs index,https://huggingface.co/docs/accelerate/index,Distributed training (Accelerate); Multi-GPU launch; Mixed-precision tuning,Fine-tuning & Post-training; Orchestration & Workflows,Hugging Face; Accelerate; Distributed Training
hf-04,hf-open-llmops-primitives,4,Transformers pipeline tutorial (inference primitive),https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md,Transformers pipelines inference; Task selection (pipeline API); Device placement (CPU/GPU),Deployment & Serving,Hugging Face; Transformers; Inference; Ecosystem Primitives
el-01,eleutherai-eval,1,lm-evaluation-harness repository (core),https://github.com/EleutherAI/lm-evaluation-harness,Benchmark execution (lm-eval-harness); Reproducible evaluation runs,Evaluation & Testing,lm-evaluation-harness; Evaluation; Reproducibility; Benchmarking
el-02,eleutherai-eval,2,lm-evaluation-harness: Task guide (docs),https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs,Task configuration; Prompt template authoring; Custom metric configuration,Evaluation & Testing; Prompting & Prompt Management,Tasks; Prompt Templates; Evaluation
el-03,eleutherai-eval,3,lm-evaluation-harness: Models/backends config,https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/models,Model backend configuration; Serving backend integration for evals,Evaluation & Testing; Deployment & Serving,Backends; Serving Integration; Evaluation
el-04,eleutherai-eval,4,lm-evaluation-harness: CLI usage / getting started,https://github.com/EleutherAI/lm-evaluation-harness#usage,CLI-driven evaluation workflow; Batching/performance tuning for evals,Evaluation & Testing; Orchestration & Workflows,Tooling; Workflows; Evaluation
vllm-01,vllm-serving,1,vLLM docs (landing),https://docs.vllm.ai/,vLLM installation & setup; Efficient inference serving,Deployment & Serving; Performance & Load Testing,vLLM; Serving; Inference
vllm-02,vllm-serving,2,OpenAI-compatible server (docs),https://docs.vllm.ai/en/v0.13.0/serving/openai_compatible_server/,OpenAI-compatible server (vLLM); API endpoint configuration,Deployment & Serving,vLLM; Openai Compatible Api; Serving
vllm-03,vllm-serving,3,vLLM GitHub repository,https://github.com/vllm-project/vllm,Source build & dependency management; Upstream contribution workflow,Deployment & Serving,vLLM; Implementation; Deploy
vllm-04,vllm-serving,4,vLLM: serving guides index,https://docs.vllm.ai/en/latest/serving/offline_inference/,Production deployment patterns (vLLM); Operational serving configuration,Deployment & Serving; Monitoring & Observability,vLLM; Deployment; Ops; Documentation
phx-01,phoenix-observability,1,Phoenix tracing overview (LLM traces),https://arize.com/docs/phoenix/tracing/llm-traces,LLM tracing concepts; Trace analysis for debugging,Monitoring & Observability,Phoenix; Tracing; Observability
phx-02,phoenix-observability,2,Phoenix tracing: How-to (manual),https://arize.com/docs/phoenix/tracing/how-to-tracing,Tracing instrumentation; Trace export to Phoenix,Monitoring & Observability,Phoenix; Tracing; Instrumentation; Tutorial
phx-03,phoenix-observability,3,Phoenix tracing tutorial,https://arize.com/docs/phoenix/tracing/tutorial,Tracing tutorial workflow; End-to-end run debugging,Monitoring & Observability; Orchestration & Workflows,Phoenix; Debugging; Workflow; Tracing; Hands-on
phx-04,phoenix-observability,4,Phoenix docs section index (tracing/evals/datasets),https://docs.arize.com/phoenix/tracing/how-to-tracing/trace-,Observability + eval workflow navigation; Dataset/eval operations,Monitoring & Observability; Evaluation & Testing,Phoenix; Observability; Evaluation
mlf-01,mlflow-genai,1,MLflow: Evaluate prompts (Prompt Registry),https://mlflow.org/docs/latest/genai/prompt-registry/evaluate-prompts/,Prompt Registry usage; Prompt versioning; Prompt evaluation,Prompting & Prompt Management; Evaluation & Testing; Model Registry & Versioning,MLflow; Prompt management; Evaluation
mlf-02,mlflow-genai,2,Databricks docs: Evaluate & monitor AI agents (MLflow GenAI),https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/,Agent evaluation (MLflow); Agent monitoring; Trace logging,Evaluation & Testing; Monitoring & Observability; Orchestration & Workflows,Databricks; MLflow; Evaluation; Monitoring
mlf-03,mlflow-genai,3,Azure Databricks: Evaluate & monitor AI agents,https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/eval-monitor/,Agent evaluation (Azure Databricks); MLflow tracking; Agent monitoring,Evaluation & Testing; Monitoring & Observability; Orchestration & Workflows,Azure; MLflow; Evaluation; Monitoring
mlf-04,mlflow-genai,4,Databricks LLMOps glossary (context),https://www.databricks.com/glossary/llmops,LLMOps tracking concepts; Lifecycle framing (Databricks),Foundations & Lifecycle; Model Registry & Versioning,Databricks; Lifecycle; Tracking; Documentation
orlhf-01,openrlhf-posttraining,1,OpenRLHF repository (overview),https://github.com/OpenRLHF/OpenRLHF,RLHF stack setup (OpenRLHF); Post-training workflow overview,Fine-tuning & Post-training,OpenRLHF; Rlhf; Post-training; RLHF
orlhf-02,openrlhf-posttraining,2,OpenRLHF README (architecture + quickstart),https://github.com/OpenRLHF/OpenRLHF#readme,PPO/DPO workflow execution; Distributed RLHF training; Serving integration in training loops,Fine-tuning & Post-training; Deployment & Serving,OpenRLHF; Scaling; Serving Integration; PPO; DPO
orlhf-03,openrlhf-posttraining,3,OpenRLHF issues/discussions (community practices),https://github.com/OpenRLHF/OpenRLHF/issues,Operational troubleshooting (RLHF); Community practice review,Operations & Maintenance,OpenRLHF; Ops; Community
orlhf-04,openrlhf-posttraining,4,OpenRLHF releases (versioning),https://github.com/OpenRLHF/OpenRLHF/releases,Release/version tracking; Upgrade planning,Model Registry & Versioning; Operations & Maintenance,OpenRLHF; Versioning; Maintenance
lc-01,langchain-observability,1,LangChain observability (docs landing),https://docs.langchain.com/oss/python/langchain/observability,Tracing enablement (LangChain); Run inspection & debugging,Monitoring & Observability,LangChain; Observability; Tracing
lc-02,langchain-observability,2,LangChain observability: tracing setup,https://docs.langchain.com/oss/python/langchain/observability#tracing,Tracing setup; Callback instrumentation,Monitoring & Observability,LangChain; Tracing; Setup
lc-03,langchain-observability,3,LangChain observability: evaluation hooks,https://docs.langchain.com/oss/python/langchain/observability#evaluation,Evaluation hooks; Quality signal capture,Evaluation & Testing; Monitoring & Observability,LangChain; Evaluation; Monitoring
lc-04,langchain-observability,4,LangChain observability: debugging patterns,https://docs.langchain.com/oss/python/langchain/observability#debugging,Debugging patterns; Operational troubleshooting,Monitoring & Observability; Operations & Maintenance,LangChain; Debugging; Ops
li-01,llamaindex-observability,1,LlamaIndex observability guide (landing),https://developers.llamaindex.ai/python/framework/module_guides/observability/,Observability enablement (LlamaIndex); Telemetry inspection,Monitoring & Observability,LlamaIndex; Observability; Tracing
li-02,llamaindex-observability,2,LlamaIndex: OpenTelemetry / tracing integration,https://developers.llamaindex.ai/python/framework/module_guides/observability/#opentelemetry,OpenTelemetry integration; Trace export configuration,Monitoring & Observability,LlamaIndex; OpenTelemetry; Tracing; Otel
li-03,llamaindex-observability,3,LlamaIndex: Phoenix integration,https://developers.llamaindex.ai/python/framework/module_guides/observability/#phoenix-arize,Phoenix integration; RAG trace debugging,Monitoring & Observability,Phoenix; LlamaIndex; Observability; Tracing
li-04,llamaindex-observability,4,LlamaIndex: evaluation/monitoring integrations,https://developers.llamaindex.ai/python/framework/module_guides/observability/#evaluation,Evaluation/monitoring integrations; RAG quality checks,Evaluation & Testing; Monitoring & Observability,LlamaIndex; Evaluation; Monitoring; RAG
oc-01,openclaw-getting-started,1,Install OpenClaw + prerequisites,https://docs.openclaw.ai/start/getting-started,Install tooling; verify environment,Lifecycle; Deployment,Lifecycle; Deployment
oc-02,openclaw-getting-started,2,Onboard configuration (auth + gateway + channels),https://docs.openclaw.ai/start/getting-started,Run onboarding wizard; configure auth; initialize gateway settings,Automation; Security/compliance; Deployment,Automation; Security; Deployment
oc-03,openclaw-getting-started,3,Run Gateway as a service (daemon),https://docs.openclaw.ai/start/getting-started,Install/manage daemon; start/stop service,Deployment; Operations,Deployment; Operations
oc-04,openclaw-getting-started,4,Check Gateway health + troubleshoot (status/foreground),https://docs.openclaw.ai/start/getting-started,Health checks; basic debugging via foreground logs,Monitoring & Evaluation; Observability,Monitoring; Observability
oc-05,openclaw-getting-started,5,Access Control UI + send a test message (smoke test),https://docs.openclaw.ai/start/getting-started,Open dashboard; run CLI smoke test,Lifecycle; Monitoring & Evaluation,Lifecycle; Validation
