course_id,Name,Provider,Description,Topic,Skills,Roles,Web URL
azure-llmops-workshop,LLMOps Workshop (Azure),Microsoft,Hands-on labs cover the end-to-end LLMOps workflow from prompt design to monitored deployment on Azure services.,Lifecycle; Prompt Management; Eval; Observability; Deployment; Automation; Content safety,Azure AI Studio navigation; Azure OpenAI model deployment; Open-source LLM deployment (Azure); Prompt Playground experimentation; Content Safety configuration; Prompt Flow creation; Prompt Flow orchestration; Classification flow build (Prompt Flow); RAG flow build (Prompt Flow + Azure AI Search); Search index setup (Azure AI Search); Flow runtime & connections (Azure OpenAI); RAG flow evaluation (Prompt Flow); Groundedness evaluation flow; Evaluation dataset preparation; Managed endpoint deployment; Post-deploy validation; LLM monitoring enablement; Monitoring metrics preparation; Content Safety tool integration; Conditional safety routing (Prompt Flow); Operational debugging; GitHub Actions CI/CD setup; Automated PR evaluation pipeline; Golden dataset evaluation gate; CI build + CD deploy (Prompt Flow); Release workflow (branching/versioning); Model benchmarking; Quality/cost tradeoff analysis; Model selection workflow; Load testing; Latency/throughput measurement; Scaling analysis,Governance lead; Platform engineer; LLM application engineer; Technical product manager; Evaluation engineer; ML engineer; MLOps engineer,https://microsoft.github.io/llmops-workshop/
gcp-llmops-overview,LLMOps on Google Cloud,Google Cloud,Practical guidance outlines an LLMOps lifecycle and maps each phase to core Google Cloud capabilities.,Lifecycle; Data; Training/Fine-tuning; Deployment; Monitoring & evaluation; Security/compliance,LLMOps lifecycle mapping; LLMOps best-practice framing; Vertex AI Model Registry usage; Model versioning & lineage; Model lifecycle promotion; Vertex AI Pipelines orchestration; Pipeline automation; Pipeline parameterization; Vertex AI model evaluation; Metric selection; Model comparison; Vertex AI model monitoring; Drift detection; Alerting configuration,Platform engineer; Technical product manager; LLM application engineer; Evaluation engineer; MLOps engineer,https://cloud.google.com/discover/what-is-llmops
aws-llmops-workshop,LLMOps Workshop (SageMaker open LLM),AWS Samples,"Workshop materials walk through building, evaluating, and deploying LLM applications on AWS components.",Lifecycle; Post-training (QLoRA); Pipelines; Monitoring; Eval; RAG; Deployment,SageMaker Model Registry registration; Model package governance; Lifecycle state management; SageMaker fine-tuning job; QLoRA fine-tuning; Model deployment (SageMaker Hosting); SageMaker Pipelines workflow; Training pipeline automation; CI/CD integration (CodePipeline); SageMaker Model Monitor configuration; Monitoring schedule setup; BYOC monitoring; JumpStart embeddings deployment; Embedding generation; Vector store setup (OpenSearch); Document ingestion for RAG; LangChain RAG application; Vector DB querying (OpenSearch); Streamlit chatbot UI,Governance lead; Technical product manager; Platform engineer; ML engineer; LLM application engineer; MLOps engineer,https://github.com/aws-samples/llmops-workshop
aws-llmops-blog-series,Operationalize GenAI Apps with LLMOps (blog series),AWS,"Short posts explain implementation choices for prompt design, observability, and production operations on AWS.",Lifecycle; Deployment; CI/CD; Governance patterns; Scaling/cost,LLMOps reference architecture review; Service-to-lifecycle mapping; Foundation model evaluation (Clarify); Automatic vs human eval workflows; fmeval-based evaluation; Model Monitor setup; LLM monitoring (BYOC); Monitoring schedule operations; Clarify + fmeval evaluation workflow; Evaluation reporting,Platform engineer; Technical product manager; Governance lead; Evaluation engineer; LLM application engineer; MLOps engineer,https://aws.amazon.com/blogs/gametech/operationalize-generative-ai-applications-on-aws-part-i-overview-of-llmops-solution/
databricks-llmops,LLMOps on Databricks,Databricks,"Applied guidance covers data prep, fine-tuning, evaluation, and serving patterns for enterprise LLM use cases.",Lifecycle; Prompt Management; Experiment tracking; Eval/Monitoring; Deployment; Cost/perf,LLMOps lifecycle framing (Databricks); Agent evaluation (MLflow); Agent monitoring; Trace logging; LLMOps implementation (Azure Databricks); Experiment/prompt tracking (MLflow); GenAI app deployment; Governance controls; LLMOps implementation patterns (Azure Databricks); Operational lifecycle setup,Platform engineer; Technical product manager; Evaluation engineer; MLOps engineer; LLM application engineer; Governance lead,https://www.databricks.com/glossary/llmops
ibm-llmops,LLMOps (IBM Think),IBM,Reference content presents an operational approach for governing and delivering LLM applications in enterprise teams.,Lifecycle; Prompt Management; Experiment tracking; Pipelines; Governance/risk,LLMOps definition & lifecycle framing; LLMOps vs MLOps comparison; watsonx LLMOps workflow framing; Enterprise governance planning; Agentic workflow design; Agent/tool orchestration; Governance for agentic systems; AI governance & risk controls; Compliance/audit processes,Platform engineer; Technical product manager; Governance lead; LLM application engineer,https://www.ibm.com/think/topics/llmops
oracle-llmops,LLMOps (Oracle),Oracle,"Practical tutorials show how to develop, secure, and run LLM services using Oracle cloud tooling.",Lifecycle; Monitoring; Deployment; Cost/perf,LLMOps overview (Oracle); OCI service mapping for LLMOps; BYOC serving deployment (OCI Data Science); Containerized LLM serving (vLLM); Observability dashboarding; Logs/metrics/traces analysis; Alerting configuration; CI/CD observability; DevOps pipeline performance monitoring,MLOps engineer; Platform engineer; Technical product manager; ML engineer,https://www.oracle.com/artificial-intelligence/llmops/
hf-open-llmops-primitives,Open LLMOps Primitives (HF ecosystem),Hugging Face,"Open-source lessons explain foundational building blocks for training, adaptation, and lifecycle management.",Ecosystem primitives; PEFT; Distributed training; Post-training,RLHF training (TRL); DPO preference optimization; Transformers + TRL integration; PEFT fine-tuning; LoRA adapter training; Adapter merge/export; Distributed training (Accelerate); Multi-GPU launch; Mixed-precision tuning; Transformers pipelines inference; Task selection (pipeline API); Device placement (CPU/GPU),ML engineer; LLM application engineer; Platform engineer; Technical product manager,https://huggingface.co/docs/trl/index
eleutherai-eval,LLMOps Evaluation Harness,EleutherAI,Community resources focus on rigorous evaluation workflows and reproducible benchmarking for language models.,Evaluation; Regression testing; Reproducibility,Benchmark execution (lm-eval-harness); Reproducible evaluation runs; Task configuration; Prompt template authoring; Custom metric configuration; Model backend configuration; Serving backend integration for evals; CLI-driven evaluation workflow; Batching/performance tuning for evals,Evaluation engineer; LLM application engineer; Platform engineer,https://github.com/EleutherAI/lm-evaluation-harness
vllm-serving,vLLM Serving & OpenAI-Compatible API,vLLM Project,"Technical guides cover efficient inference serving, scaling strategies, and deployment integrations for vLLM.",Deployment/Serving; Performance; OpenAI-compatible endpoints,vLLM installation & setup; Efficient inference serving; OpenAI-compatible server (vLLM); API endpoint configuration; Source build & dependency management; Upstream contribution workflow; Production deployment patterns (vLLM); Operational serving configuration,Evaluation engineer; Platform engineer; ML engineer; LLM application engineer; MLOps engineer,https://docs.vllm.ai/
phoenix-observability,Phoenix Observability & Evals,Arize Phoenix,Examples demonstrate tracing and debugging techniques for improving LLM application reliability over time.,Observability; Tracing; Eval; Experiments,LLM tracing concepts; Trace analysis for debugging; Tracing instrumentation; Trace export to Phoenix; Tracing tutorial workflow; End-to-end run debugging; Observability + eval workflow navigation; Dataset/eval operations,MLOps engineer; LLM application engineer; Evaluation engineer; Platform engineer,https://arize.com/docs/phoenix/
mlflow-genai,MLflow for GenAI (Prompt Registry + Eval),MLflow,"Documentation describes how to manage prompts, models, and deployments with reproducible MLflow workflows.",Prompt Management; Eval; Tracking,Prompt Registry usage; Prompt versioning; Prompt evaluation; Agent evaluation (MLflow); Agent monitoring; Trace logging; Agent evaluation (Azure Databricks); MLflow tracking; LLMOps tracking concepts; Lifecycle framing (Databricks),Evaluation engineer; LLM application engineer; MLOps engineer; Platform engineer; Technical product manager,https://mlflow.org/docs/latest/genai/prompt-registry/evaluate-prompts/
openrlhf-posttraining,OpenRLHF (RLHF pipelines),OpenRLHF,Implementation notes explain post-training methods such as preference optimization and reinforcement learning from feedback.,Post-training (RLHF); Scaling/cost; Serving integration,RLHF stack setup (OpenRLHF); Post-training workflow overview; PPO/DPO workflow execution; Distributed RLHF training; Serving integration in training loops; Operational troubleshooting (RLHF); Community practice review; Release/version tracking; Upgrade planning,LLM application engineer; ML engineer; Platform engineer; Technical product manager,https://github.com/OpenRLHF/OpenRLHF
langchain-observability,LangChain Observability (OSS),LangChain,"Guides show how to instrument chains and agents to inspect quality, latency, and failure patterns.",Observability; Tracing; Agent workflow debugging,Tracing enablement (LangChain); Run inspection & debugging; Tracing setup; Callback instrumentation; Evaluation hooks; Quality signal capture; Debugging patterns; Operational troubleshooting,MLOps engineer; Evaluation engineer; Platform engineer,https://docs.langchain.com/oss/python/langchain/observability
llamaindex-observability,LlamaIndex Observability (OSS),LlamaIndex,"Tutorials detail observability patterns for retrieval workflows, response quality, and system performance.",Observability; Tracing; RAG workflow debugging,Observability enablement (LlamaIndex); Telemetry inspection; OpenTelemetry integration; Trace export configuration; Phoenix integration; RAG trace debugging; Evaluation/monitoring integrations; RAG quality checks,MLOps engineer; LLM application engineer; Evaluation engineer,https://developers.llamaindex.ai/python/framework/module_guides/observability/
openclaw-getting-started,OpenClaw Getting Started,OpenClaw,Starter materials introduce local-first agent workflows and the core operational patterns behind the framework.,Lifecycle; Deployment; Automation; Basic Monitoring; Security Configuration,Install tooling; verify environment; Run onboarding wizard; configure auth; initialize gateway settings; Install/manage daemon; start/stop service; Health checks; basic debugging via foreground logs; Open dashboard; run CLI smoke test,Platform engineer; Technical product manager; Governance lead; MLOps engineer; Evaluation engineer,https://docs.openclaw.ai/start/getting-started
